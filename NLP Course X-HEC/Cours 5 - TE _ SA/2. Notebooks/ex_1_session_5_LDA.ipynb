{"cells":[{"cell_type":"markdown","metadata":{"id":"Rjn3PExJLW8T"},"source":["# XHEC - Session 5-1\n","\n","\n","-----------------\n","\n","## Topic Extraction : LDA Implementation  "]},{"cell_type":"markdown","metadata":{"id":"W_WXxxh1LW8X"},"source":["In this session we will build an LDA from scratch"]},{"cell_type":"markdown","metadata":{"id":"BSJQm0G1LW8Y"},"source":["### Import libraries "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_SKho6RwLW8Y","executionInfo":{"status":"ok","timestamp":1678478387256,"user_tz":-60,"elapsed":728,"user":{"displayName":"Kaan Çaylan","userId":"10373367599679356992"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"40ad568e-de71-4bf6-de0c-ccede36a1093"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import itertools\n","import random\n","from nltk import word_tokenize\n","\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"MDweMCGQLW8a"},"source":["### Create documents "]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":325,"status":"ok","timestamp":1678478398939,"user":{"displayName":"Kaan Çaylan","userId":"10373367599679356992"},"user_tz":-60},"id":"hR0-IvHgLW8a"},"outputs":[],"source":["rawdocs = ['eat turkey on turkey day holiday',\n","           'i like to eat cake on holiday',\n","           'turkey trot race on thanksgiving holiday',\n","           'snail race the turtle',\n","           'time travel space race',\n","           'movie on thanksgiving',\n","           'movie at air and space museum is cool movie',\n","           'aspiring movie star']\n","\n","rawdocs = [word_tokenize(sentence) for sentence in rawdocs]\n","rawdocs = [[word for word in sentence] for sentence in rawdocs] # TO DO : Create a list of list of words in each sentence (Split by whitespace)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1678478402748,"user":{"displayName":"Kaan Çaylan","userId":"10373367599679356992"},"user_tz":-60},"id":"dzkJ1XYmLW8b","outputId":"7b6380c5-0c4a-4151-c7f9-2fef29c0de96"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['eat', 'turkey', 'on', 'turkey', 'day', 'holiday'],\n"," ['i', 'like', 'to', 'eat', 'cake', 'on', 'holiday'],\n"," ['turkey', 'trot', 'race', 'on', 'thanksgiving', 'holiday'],\n"," ['snail', 'race', 'the', 'turtle'],\n"," ['time', 'travel', 'space', 'race'],\n"," ['movie', 'on', 'thanksgiving'],\n"," ['movie', 'at', 'air', 'and', 'space', 'museum', 'is', 'cool', 'movie'],\n"," ['aspiring', 'movie', 'star']]"]},"metadata":{},"execution_count":6}],"source":["rawdocs"]},{"cell_type":"markdown","metadata":{"id":"JrWUkZA-LW8c"},"source":["### Set parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V3U26nZnLW8c"},"outputs":[],"source":["K = 2 #Number of topic\n","alpha = 0.1 #Hyperparameter alpha\n","eta = 0.1 #Hyperparameter eta\n","iterationNb = 3 #Number of iterations"]},{"cell_type":"markdown","metadata":{"id":"HoY5sMADLW8d"},"source":["### Convert to a numerical problem "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8h8-A19aLW8d"},"outputs":[],"source":["# TO DO: Create a dictionnary {id1: word1, id2: word2}\n","vocab = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1678117410612,"user":{"displayName":"Idriss Bennis","userId":"13669825873440235005"},"user_tz":-60},"id":"isPDl6VoLW8d","outputId":"665d9b03-7837-4a02-edf2-3ed540b483b4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{}"]},"metadata":{},"execution_count":6}],"source":["vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSnxUieZLW8e"},"outputs":[],"source":["#TO DO: Swap word for id in each document\n","document = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1678117414419,"user":{"displayName":"Idriss Bennis","userId":"13669825873440235005"},"user_tz":-60},"id":"Z_VsANXMLW8e","outputId":"87719d42-4737-40d2-deb8-e4b2d82b1d28"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":8}],"source":["document"]},{"cell_type":"markdown","metadata":{"id":"nE3_84PzLW8e"},"source":["### Create the topic-word matrix"]},{"cell_type":"markdown","source":["wordTopicMatrix:\n","- each line is related to a word\n","- each column is related to a Topic\n","- The cell (w,t) is related to the number of time that the word w has been assigned to the topic t\n","   \n","In order to create wordTopicMatrix we need to assign topic for each word (topicAssignmentList)\n"," \n","topicAssignmentList has the same length as document and we randomly assign a topic to each of the word. So the (i, j) element of topicAssignmentList correspond to the topic assign to the j-th word of the i-th document "],"metadata":{"id":"HWKz2jHCjdEI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yT6SIxoFLW8f"},"outputs":[],"source":["def initialiseWordTopicMatrix(vocab, document, K):\n","    #TO DO: Initialise the topic-word count matrix with 0 - shape (numberOftopic, numberOfword)\n","    TopicWordMatrix = []\n","    #Randomly assign topic for each word in each document\n","    topicAssignmentList = [[random.randint(0,K-1) for i in range(len(doc))] for doc in document]\n","\n","    for iDoc, doc in enumerate(document): #For all document\n","        for iToken, wordId in enumerate(doc): #For all token\n","            #TO DO : Find the topic of the given token\n","            #TO DO : Update the wordTopicMatrix\n","    return TopicWordMatrix, topicAssignmentList"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C69DTSlZLW8f"},"outputs":[],"source":["TopicWordMatrix, topicAssignmentList = initialiseWordTopicMatrix(vocab, document, K)"]},{"cell_type":"code","source":["TopicWordMatrix"],"metadata":{"id":"pPRnXa2ZkV7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topicAssignmentList"],"metadata":{"id":"hSXMb1NHkXab"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VFv-up0bLW8g"},"source":["### Create the document-topic matrix "]},{"cell_type":"markdown","source":["DocumentTopicMatrix:\n","\n","- each line is related to a document\n","- each column is related to a topic\n","- cell (d, t) is related to the number of word in the document d that hs been assigned to the topic t"],"metadata":{"id":"WSwkq_4wkc85"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6W6tzpxrLW8g"},"outputs":[],"source":["def initialiseDocumentTopicMatrix(topicAssignmentList, document):\n","    #TO DO: Initialise document topic matrix with 0 - shape (number of document, number of topic)\n","    documentTopicMatrix = []\n","    for iDoc in range(len(document)):\n","        for iTopic in range(K):\n","            #TO DO : Update document matrix topic according to topicAssignmentList\n","    return documentTopicMatrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFY7h2gOLW8g"},"outputs":[],"source":["documentTopicMatrix = initialiseDocumentTopicMatrix(topicAssignmentList, document)"]},{"cell_type":"code","source":["documentTopicMatrix"],"metadata":{"id":"IF6ns40-lA31"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VzFLFUkzLW8h"},"source":["### LDA iterations "]},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?export=view&id=1rREuNnVIZIpPIdvC37hFEWcER6puzmYa\"/></center>\n","\n","- Focus each iteration:\n","    - For each document:\n","        - For each word:\n","            \n","            - Focus on the current word w -> Find the topic assign to the word w in topicAssignmentList \n","            - Now we want to forget the topic initialy assigned to the word w in order to assign a better topic \n","                - decrement documentTopicMatrix and TopicWordMatrix \n","                - ex: For the i-th word of the j-th document, if the word was assigned to the topic 1, we'll decrement the (j, 1) element of documentTopicMatrix and the (1, word_id) element of TopicWordMatrix because the word is not assigned to the first topic anymore\n","            - Implement Gibbs sampling algorithm: find the probabilites that the word w will be assign to each topic\n","            - Build the multinomial law with the previously found probability and simulate it with a weighted random\n","            - Re-assign the word w to the new topic\n","                - increment documentTopicMatrix and TopicWordMatrix"],"metadata":{"id":"_piiRsnGlLqU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8ofRiRhLW8h"},"outputs":[],"source":["def ldaModel(K, alpha, eta, iterationNb, document, vocab, TopicWordMatrix, topicAssignmentList, documentTopicMatrix):\n","    #For each iteration\n","        #For each document\n","            #For each word in the document\n","                #TO DO: Find the initial topic for the token\n","\n","                #TO DO: Focus of the i-th Token - decrement the documentTopicMatrix and TopicWordMatrix\n","                \n","                #Gibbs-Sampling - For each topic\n","                    #TO DO: Find the probability\n","                \n","                #TO DO: Simulate the multinomial law to find the new topic\n","                \n","                #TO DO: Re-assign topic\n","    #Normalize matrix\n","    documentTopicMatrix = ((documentTopicMatrix+alpha).T/(documentTopicMatrix+alpha).sum(axis=1)).T\n","    TopicWordMatrix = ((TopicWordMatrix+alpha).T/(TopicWordMatrix+alpha).sum(axis=1)).T\n","    return documentTopicMatrix, TopicWordMatrix, topicAssignmentList\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90RLhhOPLW8h"},"outputs":[],"source":["documentTopicMatrixUpdate, TopicWordMatrixUpdate, topicAssignmentListUpdate = ldaModel(K, alpha, eta, iterationNb, document, vocab, TopicWordMatrix, topicAssignmentList, documentTopicMatrix)\n"]},{"cell_type":"code","source":["documentTopicMatrixUpdate"],"metadata":{"id":"1M41MPbHlYB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TopicWordMatrixUpdate"],"metadata":{"id":"n2rIk_NglazQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topicAssignmentListUpdate"],"metadata":{"id":"KhRQUs_bldpT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bvSSRPnlLW8j"},"source":["### Show topic "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDXsloU9LW8j"},"outputs":[],"source":["#Find the most representative word for a topic \n","#Form: \"word1\"*coeff1 + \"word2\"*coeff2+... \n","\n","def displayTopic(TopicWordMatrixUpdate, vocab, nb_word):\n","    vocab = {v: k for k, v in vocab.items()} #Swap id and value to have a dict {id: \"word\"}\n","    for topicNb, wordPerTopic in enumerate(TopicWordMatrixUpdate):\n","        print(f\"\\n>>> Topic {topicNb}\")\n","        TopicWordMatrixSeries = pd.Series(wordPerTopic).sort_values(ascending=False) \n","        wordIds = TopicWordMatrixSeries.index\n","        topicToString = []\n","        for i in range(nb_word):\n","            topicToString.append(f\"{vocab[wordIds[i]]}*{round(TopicWordMatrixSeries[wordIds[i]],2)}\")\n","        print('+'.join(topicToString))"]},{"cell_type":"code","source":["displayTopic(TopicWordMatrixUpdate, vocab, 6)"],"metadata":{"id":"MvE5579tl1pM"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"X-HEC-NLP_2023-JXD82c8h","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"vscode":{"interpreter":{"hash":"7ea335811bd7b51820f2c8fb899bfe2ead329f7b1ad34317ed070eed1007e2fc"}}},"nbformat":4,"nbformat_minor":0}