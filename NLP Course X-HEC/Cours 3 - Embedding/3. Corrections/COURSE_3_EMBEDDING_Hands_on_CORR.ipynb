{"cells":[{"cell_type":"markdown","source":["#  **Hands-on of course 3 : Embedding**"],"metadata":{"id":"O7GwbGzbltTL"}},{"cell_type":"markdown","metadata":{"id":"Qc4Q9tsi9xS6"},"source":["# **PART 1 : LSA Demonstrator**\n","\n","In this tutorial, you will learn how to use Latent Semantic Analysis to either discover hidden topics from given documents in an unsupervised way \n","Later you'll use LSA values as a feature vectors to classify document with known document categories."]},{"cell_type":"markdown","metadata":{"id":"9qArYNCS9xTA"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7LgQr8KW0s1"},"outputs":[],"source":["import os, ssl\n","if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n","    ssl._create_default_https_context = ssl._create_unverified_context"]},{"cell_type":"code","source":["!pip install gensim==4.1.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rjoKQ4lno_8G","outputId":"ed797347-6d1a-4355-94cf-b5b0e334f7ed","executionInfo":{"status":"ok","timestamp":1675960399868,"user_tz":-60,"elapsed":15628,"user":{"displayName":"Harshit Shangari","userId":"13635680920417762968"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gensim==4.1.2\n","  Downloading gensim-4.1.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.1.2) (6.3.0)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from gensim==4.1.2) (1.21.6)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.1.2) (1.7.3)\n","Installing collected packages: gensim\n","  Attempting uninstall: gensim\n","    Found existing installation: gensim 3.6.0\n","    Uninstalling gensim-3.6.0:\n","      Successfully uninstalled gensim-3.6.0\n","Successfully installed gensim-4.1.2\n"]}]},{"cell_type":"code","source":["import gensim\n","from gensim.test.utils import get_tmpfile\n","print(gensim.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9WGpJaypDy3","outputId":"6a938a45-eaec-4121-c287-735011e7cf5e","executionInfo":{"status":"ok","timestamp":1675960401618,"user_tz":-60,"elapsed":1765,"user":{"displayName":"Harshit Shangari","userId":"13635680920417762968"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.1.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g3JLBYFo9xTC","outputId":"014447be-18ba-4df8-ce9c-b572119a0d23","executionInfo":{"status":"ok","timestamp":1675960404412,"user_tz":-60,"elapsed":2822,"user":{"displayName":"Harshit Shangari","userId":"13635680920417762968"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}],"source":["#import modules\n","import os\n","import pandas as pd\n","import numpy as np\n","from string import punctuation\n","\n","import nltk\n","from nltk import WordNetLemmatizer, word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","import matplotlib.pyplot as plt\n","\n","nltk.download(\"stopwords\")\n","nltk.download('punkt')\n","nltk.download(\"wordnet\")\n","nltk.download('omw-1.4')"]},{"cell_type":"markdown","metadata":{"id":"ibAWJdKl9xTD"},"source":["## Preprocessing function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DsrwhdiL9xTD"},"outputs":[],"source":["stop_words = nltk.corpus.stopwords.words(\"english\")\n","stop_char = stop_words + list(punctuation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wAzqpqIL9xTE"},"outputs":[],"source":["def preprocessing(sentence):\n","    \"\"\" Basic processing of a document, word by word. \n","    Outputs a list of processed tokens\n","    \"\"\"\n","    # Tokenization\n","    tokens = word_tokenize(sentence)\n","    # stopwords + lowercase\n","    tokens = [token.lower().replace(\"'\", \"\") for token in tokens if token.lower() not in stop_char]\n","    \n","    Lemmatizer = WordNetLemmatizer()\n","    tokens = [Lemmatizer.lemmatize(token) for token in tokens]\n","    \n","    # Deleting words with  only one caracter\n","    tokens = [token for token in tokens if len(token)>2]\n","    [word for word in words if len(token)>2]\n","    \n","    return tokens"]},{"cell_type":"markdown","metadata":{"id":"l-HYHYXc9xTE"},"source":["## A. Example on few sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7eD391a9xTE","outputId":"5d234904-f02f-4200-ab6a-23d3ac36d9c1","executionInfo":{"status":"ok","timestamp":1675950673654,"user_tz":-60,"elapsed":11,"user":{"displayName":"Sophie MARCHAL","userId":"09239647468066187959"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I believe cats are better animals than dogs, I love cats !',\n"," 'I saw this movie named cats, it was quite bad',\n"," 'The cat jumped over the gate',\n"," 'Artificial intelligence is fun',\n"," 'Business and data science / artificial intelligence combination is the key',\n"," 'Data science is the future and data is the new black gold']"]},"metadata":{},"execution_count":51}],"source":["docA = 'I believe cats are better animals than dogs, I love cats !'\n","docB = 'I saw this movie named cats, it was quite bad'\n","docC = 'The cat jumped over the gate'\n","\n","docD = 'Artificial intelligence is fun'\n","docE = 'Business and data science / artificial intelligence combination is the key'\n","docF = 'Data science is the future and data is the new black gold'\n","docs = [docA, docB, docC, docD, docE, docF]\n","docs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-CudqU8W0s-","outputId":"2c0fb42f-55a6-4ecc-996a-566f252275da","executionInfo":{"status":"ok","timestamp":1675950673654,"user_tz":-60,"elapsed":10,"user":{"displayName":"Sophie MARCHAL","userId":"09239647468066187959"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the', 'code', 'will', 'delete', '', 'but', 'not', 'km']"]},"metadata":{},"execution_count":52}],"source":["import re\n","# We will separate each sentence into tokens\n","def strip_digit(tokens):\n","    tokens = [re.sub(\"\\d+\", \"\", token) for token in tokens ]\n","    tokens = [token for token in tokens if len(token)!=\"\"]\n","    return tokens\n","\n","strip_digit([\"the\",'code',\"will\", \"delete\", \"100\", \"but\", \"not\",\"100km\"])"]},{"cell_type":"markdown","metadata":{"id":"9MfggVFD9xTF"},"source":["### Preprocessing"]},{"cell_type":"markdown","source":["**Question 1 : Complete the code in order to preprocess docs**\n"],"metadata":{"id":"nP0O5KpMd39c"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"P8nFauxh9xTF","outputId":"fb13b5d9-25fd-47f7-f10d-b9f7798dab71","executionInfo":{"status":"error","timestamp":1675950673654,"user_tz":-60,"elapsed":10,"user":{"displayName":"Sophie MARCHAL","userId":"09239647468066187959"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-79b799ebfa85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m### START CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mpreprocessed_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;31m### END CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0msimple_clean_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-073a4a052e16>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Deleting words with  only one caracter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"]}],"source":["\n","simple_clean_docs = []\n","for doc in docs: \n","  ### START CODE HERE\n","  preprocessed_doc = preprocessing(doc)\n","  ### END CODE HERE\n","  simple_clean_docs.append(preprocessed_doc)\n","\n","simple_corpus = [' '.join(sentence) for sentence in simple_clean_docs]\n","simple_corpus"]},{"cell_type":"markdown","metadata":{"id":"10aFwzhs9xTF"},"source":["### TF-IDF vectorization\n","To convert text data in a document-term matrix, we are goint to use `TfidfVectorizer` from `sklearn` library"]},{"cell_type":"markdown","source":["**Question 2 : Complete the code in order to apply the TF IDF vectorization to simple corpus**"],"metadata":{"id":"Nu-rDWO0ehu9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nK6F7Hd49xTG"},"outputs":[],"source":["# START CODE HERE\n","simple_vectorizer = TfidfVectorizer() # Initialization of Tf IDF\n","simple_vect_corpus = simple_vectorizer.fit_transform(simple_corpus) # apply tfidf to simple corpus\n","# END CODE HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"paszvDMa9xTH"},"outputs":[],"source":["simple_dictionary = np.array(simple_vectorizer.get_feature_names())\n","simple_df_tfidf = pd.DataFrame(simple_vect_corpus.todense(), columns = simple_dictionary)\n","simple_df_tfidf.head()"]},{"cell_type":"markdown","metadata":{"id":"IDJ6LQ2B9xTI"},"source":["### Singular Value Decomposition"]},{"cell_type":"markdown","metadata":{"id":"25fWHE9f9xTI"},"source":["**Question 3 : Apply SVD**\n","\n","To perform Singular Value Decomposition, you can use `TruncatedSVD`. You must specify the number of topics/latent features you are expecting. Default value is set to 2. Here we will keep 2 as number of components as we are expecting to discover 2 topics regarding this corpus. Later, you'll see how to optimize this number."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Sm8e7v09xTI"},"outputs":[],"source":["# START CODE HERE\n","simple_svd = TruncatedSVD(n_components=2) # Initialize SVD with n_components = 2\n","simple_lsa = simple_svd.fit_transform(simple_df_tfidf) # Apply SVD to simple_tf_idf\n","# END CODE HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPttyOQ49xTI","scrolled":true},"outputs":[],"source":["simple_topic_encoded_df = pd.DataFrame(simple_lsa, columns=['topic_1', 'topic_2'])\n","simple_topic_encoded_df['corpus'] = simple_corpus\n","simple_topic_encoded_df"]},{"cell_type":"markdown","metadata":{"id":"FefSuIU69xTJ"},"source":["### Deep dive into dictionary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mSXq6rf9xTJ"},"outputs":[],"source":["simple_dictionary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpDyVjBY9xTJ"},"outputs":[],"source":["simple_encoding_matrix = pd.DataFrame(simple_svd.components_, index=['topic_1', 'topic_2'], columns=simple_dictionary).T\n","simple_encoding_matrix"]},{"cell_type":"markdown","metadata":{"id":"2XA25ifZ9xTK"},"source":["**Question 4 : What are the top words for each topics ?** "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0m5LF5NU9xTL"},"outputs":[],"source":["# START CODE HERE\n","simple_encoding_matrix['abs_topic_1'] = np.abs(simple_encoding_matrix['topic_1']) # GET ABSOLUTE VALUE OF COLUMN TOPIC 1\n","simple_encoding_matrix['abs_topic_2'] = np.abs(simple_encoding_matrix['topic_2']) # GET ABSOLUTE VALUE OF COLUMN TOPIC 2\n","# END CODE HERE\n","simple_encoding_matrix.sort_values('abs_topic_1', ascending=False)\n"]},{"cell_type":"code","source":["simple_encoding_matrix.sort_values('abs_topic_2', ascending=False)"],"metadata":{"id":"YeiCEdGYhtr6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8sdZaE-p9xTM"},"source":["## B. On larger corpus\n","We will use the corpus NLTK Gutenburg that includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/.\n","\n","We will use the two books :\n","\n","1.   Alice in Wonderland of Lewis Carroll\n","2.   Hamlet of Shakespeare\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0TjbWk4M9xTM","executionInfo":{"status":"error","timestamp":1675960373185,"user_tz":-60,"elapsed":28,"user":{"displayName":"Harshit Shangari","userId":"13635680920417762968"}},"colab":{"base_uri":"https://localhost:8080/","height":200},"outputId":"75b8b623-09ff-4aef-ea2e-0bb774f209e8"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-72b56de7a557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gutenberg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0malice_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'carroll-alice.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhamlet_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shakespeare-hamlet.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"]}],"source":["nltk.download('gutenberg')\n","alice_raw = nltk.corpus.gutenberg.raw('carroll-alice.txt')\n","hamlet_raw = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')"]},{"cell_type":"markdown","metadata":{"id":"eOnR4JFb9xTM"},"source":["### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrMYHt4B9xTM"},"outputs":[],"source":["alice_sentences = nltk.sent_tokenize(alice_raw)\n","\n","alice_sentence_clean = []\n","for sent in alice_sentences:\n","    if len(sent)>0:\n","        alice_sentence_clean.append(preprocessing(sent))\n","    \n","print(\"Number of sentences after cleaning:\", len(alice_sentence_clean))\n","alice_sentence_clean[50]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L02r2ypu9xTM"},"outputs":[],"source":["hamlet_sentences = nltk.sent_tokenize(hamlet_raw)\n","\n","hamlet_sentence_clean = []\n","for sent in hamlet_sentences:\n","    if len(sent)>0:\n","        hamlet_sentence_clean.append(preprocessing(sent))\n","    \n","print(\"Number of sentences after cleaning:\", len(hamlet_sentence_clean))\n","hamlet_sentence_clean[50]"]},{"cell_type":"markdown","metadata":{"id":"DjKjNNdG9xTN"},"source":["### TF-IDF vectorization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJtc_CiX9xTN"},"outputs":[],"source":["corpus_alice = pd.concat([pd.Series((' '.join(sentence) for sentence in alice_sentence_clean), name='sentence'), \n","                          pd.Series(np.ones(len(alice_sentence_clean)), name='is_Alice')], axis=1)\n","\n","corpus_hamlet = pd.concat([pd.Series((' '.join(sentence) for sentence in hamlet_sentence_clean), name='sentence'), \n","                          pd.Series(np.zeros(len(hamlet_sentence_clean)), name='is_Alice')], axis=1)\n","\n","corpus = pd.concat([corpus_alice, corpus_hamlet]).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RDhyonz89xTN"},"outputs":[],"source":["corpus"]},{"cell_type":"markdown","source":["**Question 5 : Apply TF IDF to sentences in corpus**"],"metadata":{"id":"8qy3991yglm_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RT-5Xd-M9xTN"},"outputs":[],"source":["vectorizer = TfidfVectorizer(min_df=3)\n","\n","# START CODE HERE \n","vect_corpus = vectorizer.fit_transform(corpus['sentence'])\n","\n","# END CODE HERE\n","\n","dictionary = np.array(vectorizer.get_feature_names())\n","df_tfidf = pd.DataFrame(vect_corpus.todense(), columns = dictionary)\n","df_tfidf.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"rVmFVfLj9xTN"},"source":["### Sparsity of the matrix"]},{"cell_type":"markdown","metadata":{"id":"ifWM_D4q9xTN"},"source":["**Question 6 : What is the dimension of the tf-idf matrix ? What each dimension represents ?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cmjfq6Gz9xTO"},"outputs":[],"source":["# START CODE HERE\n","df_tfidf.shape\n","# END CODE HERE\n"]},{"cell_type":"markdown","source":["**Question 7 : What are the words that have in average the highest frequency in the corpus**"],"metadata":{"id":"zB6MaJ1ohlS9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VkXcBIG9xTO"},"outputs":[],"source":["# START CODE HERE\n","df_tfidf_mean = df_tfidf.mean()\n","\n","# END CODE HERE\n","df_tfidf_mean = df_tfidf_mean.sort_values(ascending=False).to_frame(name='tfidf mean')\n","df_tfidf_mean[:15].plot(kind='bar')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"m1uK8qpf9xTO"},"source":["### Singular Value Decomposition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_s8_WQa9xTO"},"outputs":[],"source":["svd = TruncatedSVD(n_components=2)\n","lsa = svd.fit_transform(df_tfidf)"]},{"cell_type":"markdown","source":["**Question 8 : Exclude sentences with length <= 15**\n","\n"],"metadata":{"id":"f4BtM-BZi1dN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FP82lgaG9xTO"},"outputs":[],"source":["topic_encoded_df = pd.DataFrame(lsa, columns=['topic_1', 'topic_2'])\n","topic_encoded_df['sentence'] = corpus['sentence']\n","topic_encoded_df['is_Alice'] = corpus['is_Alice']\n","\n","# START CODE HERE\n","topic_encoded_df['len'] = topic_encoded_df['sentence'].str.split().str.len() # compute length of each sentence ( number of words)\n","topic_encoded_df[topic_encoded_df['len']>15] # Filter on sentences with length > 15\n","# END CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"_JTXNCat9xTP"},"source":["### Deep dive into Dictioniary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZWbiUlT9xTP"},"outputs":[],"source":["dictionary[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGTg4IdD9xTQ"},"outputs":[],"source":["encoding_matrix = pd.DataFrame(svd.components_, index=['topic_1', 'topic_2'], columns=dictionary).T\n","encoding_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5XAm13a9xTQ"},"outputs":[],"source":["encoding_matrix['abs_topic_1'] = np.abs(encoding_matrix['topic_1'])\n","encoding_matrix['abs_topic_2'] = np.abs(encoding_matrix['topic_2'])\n","encoding_matrix.sort_values('abs_topic_1', ascending=False).head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzhd-uaw9xTQ"},"outputs":[],"source":["encoding_matrix.sort_values('abs_topic_2', ascending=False).head(10)"]},{"cell_type":"markdown","metadata":{"id":"rioscLs79xTQ"},"source":["### Plot topic encoded data"]},{"cell_type":"markdown","metadata":{"id":"-U1BC5Yu9xTQ"},"source":["We are going to represent each sentence regarding the two latent features. They are colorized regarding the `is_Alice` binary variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RN2TllI9xTR"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(10,10))\n","\n","for val in topic_encoded_df['is_Alice'].unique():\n","    topic_1 = topic_encoded_df[topic_encoded_df['is_Alice']==val]['topic_1'].values\n","    topic_2 = topic_encoded_df[topic_encoded_df['is_Alice']==val]['topic_2'].values\n","    color = \"red\" if val else \"blue\"\n","    label= \"Alice Wonderland\" if val else \"Hamlet\"\n","    ax.scatter(topic_1, topic_2, alpha=0.5, label=label)\n","    \n","ax.set_xlabel('First Topic')\n","ax.set_ylabel('Second Topic')\n","ax.axvline(linewidth=0.5)\n","ax.axhline(linewidth=0.5)\n","ax.legend()"]},{"cell_type":"markdown","metadata":{"id":"-vf68NTZ9xTR"},"source":["## Select the best number of components for SVD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkRMNrhj9xTR"},"outputs":[],"source":["svd.explained_variance_ratio_"]},{"cell_type":"markdown","metadata":{"id":"aT8Cf42F9xTR"},"source":["We will create Function Calculating Number Of Components Required To Pass Threshold. \n","\n","This function have to take in parameters a large list of explained variance ratio (number of components close from number of originally features/terms)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11FmFYWW9xTR"},"outputs":[],"source":["def select_n_components(var_ratio, var_threshold):\n","    # Set initial variance explained so far\n","    total_variance = 0.0\n","    n_components = 0\n","    \n","    # For the explained variance of each feature:\n","    for explained_variance in var_ratio:\n","        total_variance += explained_variance\n","        n_components += 1\n","    \n","        if total_variance >= var_threshold:\n","            break\n","            \n","    # Return the number of components\n","    return n_components"]},{"cell_type":"markdown","source":["**Question 9 : Select the optimal number of components to apply SVD explaining 50% of variance**"],"metadata":{"id":"fnME3ocwkBbR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-UV4TmXJ9xTR"},"outputs":[],"source":["large_svd = TruncatedSVD(n_components=df_tfidf.shape[1]-1)\n","large_lsa = large_svd.fit_transform(df_tfidf)\n","# START CODE HERE\n","threshold = 0.5\n","n_opt = select_n_components(large_svd.explained_variance_ratio_, threshold)\n","# END CODE HERE\n","print(f\"The optimal number of components to explain {threshold*100}% of the variance is {n_opt}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1AXte-m9xTS"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(10,10))\n","\n","explained_variance = pd.Series(large_svd.explained_variance_ratio_.cumsum())\n","explained_variance.plot()\n","\n","ax.xaxis.set_ticks(np.arange(0, len(explained_variance), 100))\n","\n","ax.set_xlabel('Number of Topics')\n","ax.set_ylabel('Percentage of explained variance')\n","ax.set_title('Percentage of explained variance by number of topics')"]},{"cell_type":"markdown","source":["**Question 10 : Apply SVD with optimal number of component and select for the first 10 topic the top words**"],"metadata":{"id":"yhB3xK0rlaLZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECEVTEoq9xTS"},"outputs":[],"source":["# START CODE HERE\n","optimal_svd = TruncatedSVD(n_components=n_opt)\n","optimal_lsa = optimal_svd.fit_transform(df_tfidf)\n","# END CODE HERE\n","optimal_encoding_matrix = pd.DataFrame(optimal_svd.components_, index=[f'topic_{i}' for i in range(n_opt)], columns=dictionary).T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5vQYcHE9xTT"},"outputs":[],"source":["for i in range(10):\n","  # START CODE HERE\n","    optimal_encoding_matrix[f'abs_topic_{i}'] = np.abs(optimal_encoding_matrix[f'topic_{i}']) # get Absolute value of column topic i\n","    top_words = optimal_encoding_matrix.sort_values(f'abs_topic_{i}', ascending=False).index[:5] # get top 5 words\n","\n","    # END CODE HERE\n","    print(f\"Top words for topic {i} are : \")\n","    print(top_words)\n","    print()\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"WKeSUYsa9xTU"},"source":["# **PART 2 : WORD2VEC**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQkjpfY_W0tW"},"outputs":[],"source":["corpus_alice = pd.concat([pd.Series((' '.join(sentence) for sentence in alice_sentence_clean), name='sentence'), \n","               \n","               \n","                   pd.Series(np.ones(len(alice_sentence_clean)), name='is_Alice')], axis=1)\n","\n","corpus_hamlet = pd.concat([pd.Series((' '.join(sentence) for sentence in hamlet_sentence_clean), name='sentence'), \n","                          pd.Series(np.zeros(len(hamlet_sentence_clean)), name='is_Alice')], axis=1)\n","\n","corpus = pd.concat([corpus_alice, corpus_hamlet]).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfeT0BVaW0tX"},"outputs":[],"source":["gensim_corpus = [corp.split(\" \") for corp in corpus.sentence]\n","gensim_corpus"]},{"cell_type":"code","source":["len(gensim_corpus)"],"metadata":{"id":"sgOQSSxAmJaW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gensim_corpus[0]"],"metadata":{"id":"l1tdwwm4mSjq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QPXPbz43W0tX"},"source":["__Question 11 : Create a temporary file by giving an extension and make sure you add \".model\" as extension__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_OzxsgVW0tY"},"outputs":[],"source":["# START CODE HERE\n","path = get_tmpfile(\"word2vec_lesson.model\")\n","# END CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"q7INhhCcW0ta"},"source":["__Question 12 : Instantiate your word2vec model__\n","\n","*This module implements the word2vec family of algorithms: skip-gram and CBOW models.*\n","\n","**window** = Maximum distance between the current and predicted word within a sentence.\n","\n","**min_count** = Ignores all words with total frequency lower than this.\n","\n","**workers** = Use these many worker threads to train the model (=faster training with multicore machines).\n","\n","**seed** = Seed for the random number generator."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hjrnw7nPW0ta"},"outputs":[],"source":["# START CODE HER\n","model = gensim.models.Word2Vec(window=3, min_count=5, workers=4, seed=1) \n","\n","# END CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"BftWTde0W0tb"},"source":["__Question 13 : Define the vocabulary of your model__\n","\n","Build vocabulary from a sequence of sentences. (use model.build_vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zbM8l70wW0tb"},"outputs":[],"source":["## START CODE HERE\n","model.build_vocab(gensim_corpus[:3000])\n","# END CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"nAVZelwbW0tb"},"source":["__Question 14 : Train your word2vec model__\n","\n","use model.train with epochs = 50 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jlo2tWySW0tb"},"outputs":[],"source":["## START CODE HERE\n","model.train(gensim_corpus[:3000], total_examples=model.corpus_count, epochs=50)\n","\n","# END CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"9lWvA6OqW0tc"},"source":["__Question 15 : Save your word2vec model; give the same path as in your temporary file__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5OAa22uW0tc"},"outputs":[],"source":["## START CODE HERE\n","model.save(\"word2vec_lesson.model\")\n","# END CODE HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPz0ujnlW0td"},"outputs":[],"source":["model = gensim.models.Word2Vec.load(\"word2vec_lesson.model\")"]},{"cell_type":"markdown","metadata":{"id":"jfbGDa7kW0td"},"source":["__Question 16 : Get the weight vector of a word; this is the vector (of numerical) representation of your word__ (use model.wv\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"dDsu4geZW0td"},"outputs":[],"source":["## START CODE HERE\n","list(model.wv[\"lord\"])\n","\n","# END CODE HERE"]},{"cell_type":"markdown","source":["__Question 17 : Get the 10 most similar words to \"lord\" and \"alice\"__ (use model.wv.most_similar)"],"metadata":{"id":"AE1gxpwGsdQN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XH7HbRSYW0te"},"outputs":[],"source":["# START CODE HERE / MOST SIMILAR WORDS TO \"lord\"\n","model.wv.most_similar(\"lord\", topn=10)\n","\n","# END CODE HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zJXaJV6W0te"},"outputs":[],"source":["# START CODE HERE / MOST SIMILAR WORDS TO \"alice\"\n","model.wv.most_similar(\"alice\", topn=10)\n","# END CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"jhY7CSdEW0tf"},"source":["## Create Word Embedding of Words\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"RBGaipm7W0tf"},"source":["__Question 18 : Get the embedding dict of your corpus__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QWzHX52HW0tf"},"outputs":[],"source":["embedding_matrix = dict()\n","# START CODE HERE\n","# embedding_matrix[word]= word2vec representation of the word\n","for word in model.wv.index_to_key:\n","    embedding_matrix[word] = list(model.wv[word]) # get numpy vector of a word (wv = word vector)\n","\n","# END CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"pj9fojhDW0tf"},"source":["__Question 19 : Transform it to a pandas DataFrame and look into few lines of you embedding matrix__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a50-UHqpW0tg"},"outputs":[],"source":["# START CODE HERE\n","embedding_matrix = pd.DataFrame(embedding_matrix)\n","embedding_matrix.head()\n","\n","# END CODE HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-P8Kp1G8W0th"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NiH9QFonW0th"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["9qArYNCS9xTA","ibAWJdKl9xTD","l-HYHYXc9xTE"]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":0}