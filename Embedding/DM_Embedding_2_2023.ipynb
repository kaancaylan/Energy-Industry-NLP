{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDEypOOmItyi"
      },
      "source": [
        "# Grade prediction with BERT embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnT4HuX7JUGE"
      },
      "source": [
        "In this notebook, we will use pre-trained deep learning model to process some text. We will then use the output of that model to classify the reviews we scrapped in previous courses. We will try to predict wether it was positive or negative. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nzNnspoKQYQ"
      },
      "source": [
        "Under the hood, the model is actually made up of two model.\n",
        "\n",
        "\n",
        "1.   DistilBERT processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.\n",
        "2.   The next model, a basic Logistic Regression model from scikit learn will identify if the review was positive or negative\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJjqAYOsKilN"
      },
      "source": [
        "## Installing the transformers library\n",
        "\n",
        "Let's start by installing the huggingface transformers library so we can load our deep learning NLP model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZILqaZFXIUqh",
        "outputId": "71f68d9f-2eda-4f19-e000-43f92033fe7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "p1e_arZkIUql"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import nltk\n",
        "import re\n",
        "import itertools\n",
        "import unidecode\n",
        "import tensorflow as tf\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFuqcJkdK3Wg"
      },
      "source": [
        "## Importing the dataset\n",
        "\n",
        "We'll import the dataset as we did in previous notebooks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwZ6FUS8JbJ6",
        "outputId": "ac73e86e-352b-4635-e19c-b3649c899006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Othercomputers',\n",
              " '.file-revisions-by-id',\n",
              " 'MyDrive',\n",
              " '.shortcut-targets-by-id',\n",
              " '.Trash-0']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "dirpath = \"drive\" \n",
        "os.listdir(dirpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "mXLMU3WtIUqm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# IMPORT YOUR REVIEWS FILE\n",
        "reviews = pd.read_csv(\"corpus_total.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "0GG8yiP7IUqm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 200 entries, 0 to 199\n",
            "Data columns (total 15 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   index            200 non-null    int64 \n",
            " 1   text             200 non-null    object\n",
            " 2   grade            200 non-null    int64 \n",
            " 3   date             200 non-null    object\n",
            " 4   company          200 non-null    object\n",
            " 5   n_words          200 non-null    int64 \n",
            " 6   clean_text       200 non-null    object\n",
            " 7   tokenized_text   200 non-null    object\n",
            " 8   bigrams          200 non-null    object\n",
            " 9   trigrams         200 non-null    object\n",
            " 10  wordDict         200 non-null    object\n",
            " 11  tfBOW            200 non-null    object\n",
            " 12  tfIDF            200 non-null    object\n",
            " 13  lemmatized_text  200 non-null    object\n",
            " 14  stemmed_text     200 non-null    object\n",
            "dtypes: int64(3), object(12)\n",
            "memory usage: 23.6+ KB\n"
          ]
        }
      ],
      "source": [
        "reviews.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-QTwUjyMaug"
      },
      "source": [
        "In the first place we should determine what are we considering a positive review. In this case we shall consider as positive every comment that had 4 or 5 as grade and negative otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "wANfFzAMIUqm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nan reviews:  0\n",
            "Number of Empty Reviews:  0\n"
          ]
        }
      ],
      "source": [
        "# ASSIGN A LABEL TO EACH REVIEW TO DIFFERENTIATE POSITIVE AND NEGATIVE REVIEWS\n",
        "reviews[\"sentiment\"] = reviews.grade.isin([4,5])\n",
        "# REMOVE REVIEWS WITH NULL VALUES\n",
        "print(\"Number of nan reviews: \", reviews.text.isna().sum())\n",
        "print(\"Number of Empty Reviews: \", sum([len(i) == 0 for i in reviews.text]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmTcdISzM9so"
      },
      "source": [
        "For computational purposes, we'll only use 1000 sentences which have less than 100 words in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def clean_text(text):\n",
        "    # Remove \"\\n\" characters\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    # Remove emojis and other non-letter characters (but keep punctuation)\n",
        "    text = re.sub(r'[^\\w\\s\\d\\.,;:\\?!\\'\\\"\\(\\)\\-\\$]+', ' ', text)\n",
        "    # Remove excess white space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "#Keep punctuation\n",
        "reviews[\"verbatim\"] = reviews.text.apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "Zh-vkDiMNtwB"
      },
      "outputs": [],
      "source": [
        "# Filter the reviews\n",
        "def split_reviews_per_sentence(reviews, col ):\n",
        "    reviews[\"review_sentences\"] = reviews[col].apply(\n",
        "        lambda rvw: nltk.sent_tokenize(rvw)\n",
        "    )\n",
        "    return reviews\n",
        "\n",
        "reviews = split_reviews_per_sentence(reviews, \"verbatim\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Service électricité très bien mais si Service électricité très bien mais si vous avez un surplus de règlement ça se complique.',\n",
              " \"Le mot le plus employé par les services patience C'est pas un service comptable qu ils ont à priori mais plutôt une usine à gaz... Franchement j'ai les nerfs de faire la trésorerie de Total\"]"
            ]
          },
          "execution_count": 191,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews.review_sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "rEDUy9pIIUqo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of total sentences : 732\n",
            "Number of sentences longer than 100 words:  3\n"
          ]
        }
      ],
      "source": [
        "#Save in two variables the comments and the labels\n",
        "\n",
        "sentences = list(itertools.chain(*reviews[\"review_sentences\"]))\n",
        "print(f\"Number of total sentences : {len(sentences)}\")\n",
        "\n",
        "sentence_lengths = [len(i.split(\" \")) for i in sentences]\n",
        "cond  = [i > 100 for i in sentence_lengths]\n",
        "\n",
        "long_sentences = [x for x, m in zip(sentence_lengths, cond) if m]\n",
        "print(\"Number of sentences longer than 100 words: \", len(long_sentences))\n",
        "labels = [int(i) for i in reviews.sentiment]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 3 sentences longer than 100 words. Since our corpus text which is the last 200 reviews for Total Energies contain less than 1000 sentences, we will keep these 3 sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhTEgAgqNuH4"
      },
      "source": [
        "## Loading the Pre-trained BERT model\n",
        "Let's now load a pre-trained BERT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "MX4O0N4TKhwe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f51716a8ff3143a5aae7eb9c79682a52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a8bd97452c948a48be626a4ec0b1dd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e08f3c6f76374110945913974868a915",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f17d7d6e394c42b79f70dc5d72210974",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# For DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "## Want BERT instead of distilBERT? Uncomment the following line:\n",
        "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ph0mRppN-F5"
      },
      "source": [
        "Right now, the variable model holds a pretrained distilBERT model -- a version of BERT that is smaller, but much faster and requiring a lot less memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIL8EBl2OFky"
      },
      "source": [
        "# Model 1: Preparing the Dataset\n",
        "Before we can hand our sentences to BERT, we need to do some minimal processing to put them in the format it requires."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBRoghfCOL6Z"
      },
      "source": [
        "### Tokenization\n",
        "Our first step is to tokenize the sentences -- break them up into word and subwords in the format BERT is comfortable with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "FD6VIbtjIUqo"
      },
      "outputs": [],
      "source": [
        "# Apply tokenization\n",
        "def clean_sentence(sentence, stopwords=False):\n",
        "    sentence = sentence.lower().strip() #lowercase\n",
        "    sentence = unidecode.unidecode(sentence) #remove accents from letters\n",
        "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence) #remove non-alphanumeric characters\n",
        "    sentence = re.sub(r\"\\d+\", \"\", sentence) #remove numbers\n",
        "    sentence = re.sub(r\"\\s+\",\" \",sentence) #remove whitespace\n",
        "    sentence = sentence.rstrip().lstrip() #remove space at the beginning and end if it exists\n",
        "    \n",
        "    return sentence\n",
        "\n",
        "sentences = [clean_sentence(i) for i in sentences]\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=' ', char_level=False)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "tokenized = [i.split(\" \") for i in sentences]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We created a "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "H\n",
            "\n",
            "service electricite tres bien mais si service electricite tres bien mais si vous avez un surplus de reglement ca se complique\n",
            "\n",
            "[15, 95, 26, 45, 40, 56, 15, 95, 26, 45, 40, 56, 23, 313, 9, 651, 1, 652, 73, 65, 653]\n"
          ]
        }
      ],
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(reviews[\"verbatim\"][1][0], sentences[0], sequences[0], sep=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "732"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "execution_count": 182,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIryhAglOZeR"
      },
      "source": [
        "### Padding\n",
        "After tokenization, tokenized is a list of sentences -- each sentences is represented as a list of tokens. We want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "hYnH9rohIUqp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15, 95, 26, 45, 40, 56, 15, 95, 26, 45, 40, 56, 23, 313, 9, 651, 1, 652, 73, 65, 653, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Perform the padding\n",
        "\n",
        "max_len = 0\n",
        "for i in tokenized:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "max_len\n",
        "padded = [token + [0] * (max_len - len(token)) for token in sequences]\n",
        "print(padded[0])\n",
        "\n",
        "#check if all sizes are 113\n",
        "(pd.Series([len(pad) for pad in padded]) == max_len).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbCa9CsCOmSe"
      },
      "source": [
        "Our dataset is now in the padded variable, we can view its dimensions below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "hICAGTYdIUqp"
      },
      "outputs": [],
      "source": [
        "padded = np.array(padded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpL42j8IOvfc"
      },
      "source": [
        "### Masking\n",
        "If we directly send padded to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "NB1k1ZeMIUqp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(732, 113)"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "attention_mask.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGr2t2V2O-bU"
      },
      "source": [
        "## Model 1: And Now, Deep Learning!\n",
        "Now that we have our model and inputs ready, let's run our model! \n",
        "\n",
        "The model() function runs our sentences through BERT. The results of the processing will be returned into last_hidden_states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2352"
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(tokenizer.index_word.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  15,   95,   26, ...,    0,    0,    0],\n",
              "       [   5, 1043,    5, ...,    0,    0,    0],\n",
              "       [ 269,  269,   56, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [  45,  613,    0, ...,    0,    0,    0],\n",
              "       [  97,   92,    0, ...,    0,    0,    0],\n",
              "       [  97,   92,    0, ...,    0,    0,    0]])"
            ]
          },
          "execution_count": 175,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "ARJnQ8eOIUqp"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.tensor(padded)  \n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMo4SpXiPY30"
      },
      "source": [
        "Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
        "\n",
        "We'll save those in the features variable, as they'll serve as the features to our logitics regression model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "y3nPR2mrIUqq"
      },
      "outputs": [],
      "source": [
        "features = last_hidden_states[0][:,0,:].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(732, 768)"
            ]
          },
          "execution_count": 179,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cne1LmgZPyQP"
      },
      "source": [
        "## Model 2: Train/Test Split\n",
        "Let's now split our datset into a training set and testing set (even though we're using 1,000 sentences from the reviews training set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "-EfDqA84TU_4"
      },
      "outputs": [],
      "source": [
        "# Split the data in train and test with the train_test_split function\n",
        "Xtrain, ytrain = train_test_split(features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDc9UFa5P_8G"
      },
      "source": [
        "## [Bonus] Grid Search for Parameters\n",
        "We can dive into Logistic regression directly with the Scikit Learn default parameters, but sometimes it's worth searching for the best value of the C parameter, which determines regularization strength."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg1yURnlQDRD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J39zQKd7QLqw"
      },
      "source": [
        "We now train the LogisticRegression model. If you've chosen to do the gridsearch, you can plug the value of C into the model declaration (e.g. LogisticRegression(C=5.2))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMXaIP7dTbyp"
      },
      "outputs": [],
      "source": [
        "# Fit the logistic regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeFkGIhJQREW"
      },
      "source": [
        "## Evaluating Model 2\n",
        "So how well does our model do in classifying sentences? One way is to check the accuracy against the testing dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8TLsncnTgmJ"
      },
      "outputs": [],
      "source": [
        "# Evaluate the score of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG-tur4cQc2V"
      },
      "source": [
        "How good is this score? What can we compare it against? Let's first look at a dummy classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzAn-f3kT4T8"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier()\n",
        "\n",
        "# COmpare your model with a dummy classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ildzqm6RQhGP"
      },
      "source": [
        "So our model clearly does better than a dummy classifier"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a2ab3825ac7005fb7b26f112e9c99ae62f464c629e30b0d534c3b931b6cbc3ff"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
