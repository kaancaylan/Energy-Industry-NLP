{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjn3PExJLW8T"
      },
      "source": [
        "# XHEC - Session 5-1\n",
        "\n",
        "\n",
        "-----------------\n",
        "\n",
        "## Topic Extraction : LDA Implementation  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_WXxxh1LW8X"
      },
      "source": [
        "In this session we will build an LDA from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSJQm0G1LW8Y"
      },
      "source": [
        "### Import libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SKho6RwLW8Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import random\n",
        "from nltk import word_tokenize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDweMCGQLW8a"
      },
      "source": [
        "### Create documents "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR0-IvHgLW8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "outputId": "3100b85f-c083-40d5-e865-44b2552c8d76"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3e6cf6e958c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m            'aspiring movie star']\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrawdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrawdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mrawdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrawdocs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# TO DO : Create a list of list of words in each sentence (Split by whitespace)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-3e6cf6e958c2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m            'aspiring movie star']\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrawdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrawdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mrawdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrawdocs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# TO DO : Create a list of list of words in each sentence (Split by whitespace)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "rawdocs = ['eat turkey on turkey day holiday',\n",
        "           'i like to eat cake on holiday',\n",
        "           'turkey trot race on thanksgiving holiday',\n",
        "           'snail race the turtle',\n",
        "           'time travel space race',\n",
        "           'movie on thanksgiving',\n",
        "           'movie at air and space museum is cool movie',\n",
        "           'aspiring movie star']\n",
        "\n",
        "rawdocs = [word_tokenize(sentence) for sentence in rawdocs]\n",
        "rawdocs = [[word for word in sentence] for sentence in rawdocs] # TO DO : Create a list of list of words in each sentence (Split by whitespace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzkJ1XYmLW8b",
        "outputId": "b3aab69e-98e1-49e2-c214-1eae5bea6569"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['e',\n",
              "  'a',\n",
              "  't',\n",
              "  ' ',\n",
              "  't',\n",
              "  'u',\n",
              "  'r',\n",
              "  'k',\n",
              "  'e',\n",
              "  'y',\n",
              "  ' ',\n",
              "  'o',\n",
              "  'n',\n",
              "  ' ',\n",
              "  't',\n",
              "  'u',\n",
              "  'r',\n",
              "  'k',\n",
              "  'e',\n",
              "  'y',\n",
              "  ' ',\n",
              "  'd',\n",
              "  'a',\n",
              "  'y',\n",
              "  ' ',\n",
              "  'h',\n",
              "  'o',\n",
              "  'l',\n",
              "  'i',\n",
              "  'd',\n",
              "  'a',\n",
              "  'y'],\n",
              " ['i',\n",
              "  ' ',\n",
              "  'l',\n",
              "  'i',\n",
              "  'k',\n",
              "  'e',\n",
              "  ' ',\n",
              "  't',\n",
              "  'o',\n",
              "  ' ',\n",
              "  'e',\n",
              "  'a',\n",
              "  't',\n",
              "  ' ',\n",
              "  'c',\n",
              "  'a',\n",
              "  'k',\n",
              "  'e',\n",
              "  ' ',\n",
              "  'o',\n",
              "  'n',\n",
              "  ' ',\n",
              "  'h',\n",
              "  'o',\n",
              "  'l',\n",
              "  'i',\n",
              "  'd',\n",
              "  'a',\n",
              "  'y'],\n",
              " ['t',\n",
              "  'u',\n",
              "  'r',\n",
              "  'k',\n",
              "  'e',\n",
              "  'y',\n",
              "  ' ',\n",
              "  't',\n",
              "  'r',\n",
              "  'o',\n",
              "  't',\n",
              "  ' ',\n",
              "  'r',\n",
              "  'a',\n",
              "  'c',\n",
              "  'e',\n",
              "  ' ',\n",
              "  'o',\n",
              "  'n',\n",
              "  ' ',\n",
              "  't',\n",
              "  'h',\n",
              "  'a',\n",
              "  'n',\n",
              "  'k',\n",
              "  's',\n",
              "  'g',\n",
              "  'i',\n",
              "  'v',\n",
              "  'i',\n",
              "  'n',\n",
              "  'g',\n",
              "  ' ',\n",
              "  'h',\n",
              "  'o',\n",
              "  'l',\n",
              "  'i',\n",
              "  'd',\n",
              "  'a',\n",
              "  'y'],\n",
              " ['s',\n",
              "  'n',\n",
              "  'a',\n",
              "  'i',\n",
              "  'l',\n",
              "  ' ',\n",
              "  'r',\n",
              "  'a',\n",
              "  'c',\n",
              "  'e',\n",
              "  ' ',\n",
              "  't',\n",
              "  'h',\n",
              "  'e',\n",
              "  ' ',\n",
              "  't',\n",
              "  'u',\n",
              "  'r',\n",
              "  't',\n",
              "  'l',\n",
              "  'e'],\n",
              " ['t',\n",
              "  'i',\n",
              "  'm',\n",
              "  'e',\n",
              "  ' ',\n",
              "  't',\n",
              "  'r',\n",
              "  'a',\n",
              "  'v',\n",
              "  'e',\n",
              "  'l',\n",
              "  ' ',\n",
              "  's',\n",
              "  'p',\n",
              "  'a',\n",
              "  'c',\n",
              "  'e',\n",
              "  ' ',\n",
              "  'r',\n",
              "  'a',\n",
              "  'c',\n",
              "  'e'],\n",
              " ['m',\n",
              "  'o',\n",
              "  'v',\n",
              "  'i',\n",
              "  'e',\n",
              "  ' ',\n",
              "  'o',\n",
              "  'n',\n",
              "  ' ',\n",
              "  't',\n",
              "  'h',\n",
              "  'a',\n",
              "  'n',\n",
              "  'k',\n",
              "  's',\n",
              "  'g',\n",
              "  'i',\n",
              "  'v',\n",
              "  'i',\n",
              "  'n',\n",
              "  'g'],\n",
              " ['m',\n",
              "  'o',\n",
              "  'v',\n",
              "  'i',\n",
              "  'e',\n",
              "  ' ',\n",
              "  'a',\n",
              "  't',\n",
              "  ' ',\n",
              "  'a',\n",
              "  'i',\n",
              "  'r',\n",
              "  ' ',\n",
              "  'a',\n",
              "  'n',\n",
              "  'd',\n",
              "  ' ',\n",
              "  's',\n",
              "  'p',\n",
              "  'a',\n",
              "  'c',\n",
              "  'e',\n",
              "  ' ',\n",
              "  'm',\n",
              "  'u',\n",
              "  's',\n",
              "  'e',\n",
              "  'u',\n",
              "  'm',\n",
              "  ' ',\n",
              "  'i',\n",
              "  's',\n",
              "  ' ',\n",
              "  'c',\n",
              "  'o',\n",
              "  'o',\n",
              "  'l',\n",
              "  ' ',\n",
              "  'm',\n",
              "  'o',\n",
              "  'v',\n",
              "  'i',\n",
              "  'e'],\n",
              " ['a',\n",
              "  's',\n",
              "  'p',\n",
              "  'i',\n",
              "  'r',\n",
              "  'i',\n",
              "  'n',\n",
              "  'g',\n",
              "  ' ',\n",
              "  'm',\n",
              "  'o',\n",
              "  'v',\n",
              "  'i',\n",
              "  'e',\n",
              "  ' ',\n",
              "  's',\n",
              "  't',\n",
              "  'a',\n",
              "  'r']]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "rawdocs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrWUkZA-LW8c"
      },
      "source": [
        "### Set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3U26nZnLW8c"
      },
      "outputs": [],
      "source": [
        "K = 2 #Number of topic\n",
        "alpha = 0.1 #Hyperparameter alpha\n",
        "eta = 0.1 #Hyperparameter eta\n",
        "iterationNb = 3 #Number of iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoY5sMADLW8d"
      },
      "source": [
        "### Convert to a numerical problem "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h8-A19aLW8d"
      },
      "outputs": [],
      "source": [
        "# TO DO: Create a dictionnary {id1: word1, id2: word2}\n",
        "vocab = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isPDl6VoLW8d",
        "outputId": "665d9b03-7837-4a02-edf2-3ed540b483b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSnxUieZLW8e"
      },
      "outputs": [],
      "source": [
        "#TO DO: Swap word for id in each document\n",
        "document = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_VsANXMLW8e",
        "outputId": "87719d42-4737-40d2-deb8-e4b2d82b1d28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE3_84PzLW8e"
      },
      "source": [
        "### Create the topic-word matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "wordTopicMatrix:\n",
        "- each line is related to a word\n",
        "- each column is related to a Topic\n",
        "- The cell (w,t) is related to the number of time that the word w has been assigned to the topic t\n",
        "   \n",
        "In order to create wordTopicMatrix we need to assign topic for each word (topicAssignmentList)\n",
        " \n",
        "topicAssignmentList has the same length as document and we randomly assign a topic to each of the word. So the (i, j) element of topicAssignmentList correspond to the topic assign to the j-th word of the i-th document "
      ],
      "metadata": {
        "id": "HWKz2jHCjdEI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT6SIxoFLW8f"
      },
      "outputs": [],
      "source": [
        "def initialiseWordTopicMatrix(vocab, document, K):\n",
        "    #TO DO: Initialise the topic-word count matrix with 0 - shape (numberOftopic, numberOfword)\n",
        "    TopicWordMatrix = []\n",
        "    #Randomly assign topic for each word in each document\n",
        "    topicAssignmentList = [[random.randint(0,K-1) for i in range(len(doc))] for doc in document]\n",
        "\n",
        "    for iDoc, doc in enumerate(document): #For all document\n",
        "        for iToken, wordId in enumerate(doc): #For all token\n",
        "            #TO DO : Find the topic of the given token\n",
        "            #TO DO : Update the wordTopicMatrix\n",
        "    return TopicWordMatrix, topicAssignmentList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C69DTSlZLW8f"
      },
      "outputs": [],
      "source": [
        "TopicWordMatrix, topicAssignmentList = initialiseWordTopicMatrix(vocab, document, K)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TopicWordMatrix"
      ],
      "metadata": {
        "id": "pPRnXa2ZkV7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topicAssignmentList"
      ],
      "metadata": {
        "id": "hSXMb1NHkXab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFv-up0bLW8g"
      },
      "source": [
        "### Create the document-topic matrix "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DocumentTopicMatrix:\n",
        "\n",
        "- each line is related to a document\n",
        "- each column is related to a topic\n",
        "- cell (d, t) is related to the number of word in the document d that hs been assigned to the topic t"
      ],
      "metadata": {
        "id": "WSwkq_4wkc85"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W6tzpxrLW8g"
      },
      "outputs": [],
      "source": [
        "def initialiseDocumentTopicMatrix(topicAssignmentList, document):\n",
        "    #TO DO: Initialise document topic matrix with 0 - shape (number of document, number of topic)\n",
        "    documentTopicMatrix = []\n",
        "    for iDoc in range(len(document)):\n",
        "        for iTopic in range(K):\n",
        "            #TO DO : Update document matrix topic according to topicAssignmentList\n",
        "    return documentTopicMatrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFY7h2gOLW8g"
      },
      "outputs": [],
      "source": [
        "documentTopicMatrix = initialiseDocumentTopicMatrix(topicAssignmentList, document)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentTopicMatrix"
      ],
      "metadata": {
        "id": "IF6ns40-lA31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzFLFUkzLW8h"
      },
      "source": [
        "### LDA iterations "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1rREuNnVIZIpPIdvC37hFEWcER6puzmYa\"/></center>\n",
        "\n",
        "- Focus each iteration:\n",
        "    - For each document:\n",
        "        - For each word:\n",
        "            \n",
        "            - Focus on the current word w -> Find the topic assign to the word w in topicAssignmentList \n",
        "            - Now we want to forget the topic initialy assigned to the word w in order to assign a better topic \n",
        "                - decrement documentTopicMatrix and TopicWordMatrix \n",
        "                - ex: For the i-th word of the j-th document, if the word was assigned to the topic 1, we'll decrement the (j, 1) element of documentTopicMatrix and the (1, word_id) element of TopicWordMatrix because the word is not assigned to the first topic anymore\n",
        "            - Implement Gibbs sampling algorithm: find the probabilites that the word w will be assign to each topic\n",
        "            - Build the multinomial law with the previously found probability and simulate it with a weighted random\n",
        "            - Re-assign the word w to the new topic\n",
        "                - increment documentTopicMatrix and TopicWordMatrix"
      ],
      "metadata": {
        "id": "_piiRsnGlLqU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8ofRiRhLW8h"
      },
      "outputs": [],
      "source": [
        "def ldaModel(K, alpha, eta, iterationNb, document, vocab, TopicWordMatrix, topicAssignmentList, documentTopicMatrix):\n",
        "    #For each iteration\n",
        "        #For each document\n",
        "            #For each word in the document\n",
        "                #TO DO: Find the initial topic for the token\n",
        "\n",
        "                #TO DO: Focus of the i-th Token - decrement the documentTopicMatrix and TopicWordMatrix\n",
        "                \n",
        "                #Gibbs-Sampling - For each topic\n",
        "                    #TO DO: Find the probability\n",
        "                \n",
        "                #TO DO: Simulate the multinomial law to find the new topic\n",
        "                \n",
        "                #TO DO: Re-assign topic\n",
        "    #Normalize matrix\n",
        "    documentTopicMatrix = ((documentTopicMatrix+alpha).T/(documentTopicMatrix+alpha).sum(axis=1)).T\n",
        "    TopicWordMatrix = ((TopicWordMatrix+alpha).T/(TopicWordMatrix+alpha).sum(axis=1)).T\n",
        "    return documentTopicMatrix, TopicWordMatrix, topicAssignmentList\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90RLhhOPLW8h"
      },
      "outputs": [],
      "source": [
        "documentTopicMatrixUpdate, TopicWordMatrixUpdate, topicAssignmentListUpdate = ldaModel(K, alpha, eta, iterationNb, document, vocab, TopicWordMatrix, topicAssignmentList, documentTopicMatrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentTopicMatrixUpdate"
      ],
      "metadata": {
        "id": "1M41MPbHlYB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TopicWordMatrixUpdate"
      ],
      "metadata": {
        "id": "n2rIk_NglazQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topicAssignmentListUpdate"
      ],
      "metadata": {
        "id": "KhRQUs_bldpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvSSRPnlLW8j"
      },
      "source": [
        "### Show topic "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDXsloU9LW8j"
      },
      "outputs": [],
      "source": [
        "#Find the most representative word for a topic \n",
        "#Form: \"word1\"*coeff1 + \"word2\"*coeff2+... \n",
        "\n",
        "def displayTopic(TopicWordMatrixUpdate, vocab, nb_word):\n",
        "    vocab = {v: k for k, v in vocab.items()} #Swap id and value to have a dict {id: \"word\"}\n",
        "    for topicNb, wordPerTopic in enumerate(TopicWordMatrixUpdate):\n",
        "        print(f\"\\n>>> Topic {topicNb}\")\n",
        "        TopicWordMatrixSeries = pd.Series(wordPerTopic).sort_values(ascending=False) \n",
        "        wordIds = TopicWordMatrixSeries.index\n",
        "        topicToString = []\n",
        "        for i in range(nb_word):\n",
        "            topicToString.append(f\"{vocab[wordIds[i]]}*{round(TopicWordMatrixSeries[wordIds[i]],2)}\")\n",
        "        print('+'.join(topicToString))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "displayTopic(TopicWordMatrixUpdate, vocab, 6)"
      ],
      "metadata": {
        "id": "MvE5579tl1pM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "X-HEC-NLP_2023-JXD82c8h",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "7ea335811bd7b51820f2c8fb899bfe2ead329f7b1ad34317ed070eed1007e2fc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}